{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Statistics for Experimental Physics\n",
    "## Part II: Bayesian Statistics\n",
    "### Assessed Problem Sheet 4\n",
    "\n",
    "**Due: 11 December 2024**  \n",
    "**Boris Leistedt** - B.LEISTEDT@IMPERIAL.AC.UK\n",
    "\n",
    "Based on the material originally designed by Alan Heavens\n",
    "\n",
    "Hand in by 5 pm Thursday 11th December. \n",
    "\n",
    "**Please submit a project-like report (PDF) with the results, and a copy of your code included. There are no page limits.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Cosmology with Supernovae Ia\n",
    "\n",
    "In this exercise, you will write **your own HMC code** to infer cosmological parameters from supernova Ia data. Do not use a package for the main HMC implementation, but you may use packages (e.g. corner.py) to display results.\n",
    "\n",
    "**Objectives:**\n",
    "1. Implement the cosmological distance-redshift relation for a flat universe\n",
    "2. Build a Hamiltonian Monte Carlo sampler from scratch\n",
    "3. Infer $\\Omega_m$ (matter density) and $h$ (Hubble constant) from real supernova data (Pantheon+ dataset: 1701 supernovae)\n",
    "4. Assess convergence using trace plots, autocorrelation, and effective sample size\n",
    "5. Explore how HMC parameters affect sampling efficiency\n",
    "\n",
    "**The Big Picture:** Type Ia supernovae are \"standard candles\" with known intrinsic brightness. By measuring their apparent brightness at different redshifts, we can map the distance-redshift relation, which depends on the Universe's composition ($\\Omega_m$, $\\Omega_v$, $h$). This same technique led to the 1998 Nobel Prize discovery that the Universe's expansion is accelerating, revealing the existence of dark energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### 1.1. Background and Theory\n",
    "\n",
    "#### Type Ia Supernovae: Standard Candles for Cosmology\n",
    "\n",
    "**Type Ia supernovae** are thermonuclear explosions of white dwarfs that reach the Chandrasekhar limit (~1.4 solar masses), making them excellent **standard candles**:\n",
    "- Nearly uniform peak luminosity ($L \\approx 10^{43}$ erg/s) after empirical corrections\n",
    "- Visible to cosmological distances (billions of light-years)\n",
    "- Led to the 1998 Nobel Prize discovery of accelerating cosmic expansion (Perlmutter, Schmidt, Riess)\n",
    "\n",
    "#### Cosmological Framework\n",
    "\n",
    "The Universe is expanding, causing **cosmological redshift**:\n",
    "$$z \\equiv \\frac{\\lambda_{\\text{observed}} - \\lambda_{\\text{emitted}}}{\\lambda_{\\text{emitted}}} = \\frac{a(t_{\\text{now}})}{a(t_{\\text{emission}})} - 1$$\n",
    "\n",
    "where $a(t)$ is the scale factor ($a=1$ today). \n",
    "\n",
    "**Our goal is to measure the expansion rate and composition of the universe** by observing supernovae at different redshifts. Because supernovae are standard candles with known intrinsic brightness, measuring their apparent brightness (magnitude) tells us their distance. By mapping how distance varies with redshift for many supernovae, we can reconstruct the expansion history, which is governed by the cosmological parameters $\\Omega_m$ and $h$. Different values of these parameters produce different distance-redshift curvesâ€”this is what allows us to infer cosmology from supernova observations.\n",
    "\n",
    "The expansion is governed by the **Friedmann equation**:\n",
    "$$H(z)^2 = H_0^2\\left[\\Omega_m(1+z)^3 + \\Omega_k(1+z)^2 + \\Omega_v\\right]$$\n",
    "\n",
    "**Parameters:**\n",
    "- $H_0 = 100h$ km/s/Mpc: Hubble constant ($h \\approx 0.7$, typical range 0.6-0.8)\n",
    "- $\\Omega_m \\approx 0.3$: matter density parameter (typical range 0.2-0.4)\n",
    "- $\\Omega_v \\approx 0.7$: dark energy density parameter\n",
    "- $\\Omega_k = 1 - \\Omega_m - \\Omega_v$: curvature parameter\n",
    "\n",
    "We assume a **flat universe** ($\\Omega_m + \\Omega_v = 1$, $\\Omega_k = 0$), well-supported by CMB observations.\n",
    "\n",
    "#### Distance Measurements\n",
    "\n",
    "The flux from a supernova of luminosity $L$ at **luminosity distance** $D_L$ is:\n",
    "$$f = \\frac{L}{4\\pi D_L^2}$$\n",
    "\n",
    "For a flat universe, $D_L$ is given by:\n",
    "$$D_L(z) = 3000h^{-1}(1 + z) \\int_0^z \\frac{dz'}{\\sqrt{\\Omega_m(1 + z')^3 + 1 - \\Omega_m}} \\text{ Mpc}$$\n",
    "\n",
    "Since HMC requires millions of likelihood evaluations, we use the **Pen (1999) analytical approximation** (accurate to <0.4%):\n",
    "$$D_L(z; \\Omega_m, h) = \\frac{c}{H_0}(1 + z)\\left[\\eta(1, \\Omega_m) - \\eta\\left(\\frac{1}{1 + z}, \\Omega_m\\right)\\right]$$\n",
    "\n",
    "where $c = 299792.458$ km/s and:\n",
    "$$\\eta(a, \\Omega_m) = 2\\sqrt{s^3 + 1}\\left[\\frac{1}{a^4} - 0.1540\\frac{s}{a^3} + 0.4304\\frac{s^2}{a^2} + 0.19097\\frac{s^3}{a} + 0.066941s^4\\right]^{-1/8}$$\n",
    "\n",
    "with $s^3 = (1 - \\Omega_m)/\\Omega_m$ and $a = 1/(1+z)$ (valid for $0.2 \\leq \\Omega_m \\leq 1$).\n",
    "\n",
    "Astronomers measure brightness using **magnitudes** ($m = -2.5\\log_{10}f + \\text{const}$). The **distance modulus** is:\n",
    "$$\\mu = m - M = 5\\log_{10}\\left(\\frac{D_L}{\\text{Mpc}}\\right) + 25$$\n",
    "\n",
    "Factoring out $h$ using $D_L^* \\equiv D_L(h=1)$:\n",
    "$$\\mu(z; \\Omega_m, h) = 25 - 5\\log_{10} h + 5\\log_{10}\\left(\\frac{D_L^*}{\\text{Mpc}}\\right)$$\n",
    "\n",
    "where $h$ produces a vertical shift and $\\Omega_m$ affects the curve shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 1.2. Data\n",
    "\n",
    "The data file (from the 'Pantheon+' sample - see https://arxiv.org/abs/2112.03863 for more detail) consists of data from 1701 supernovae, with a redshift and distance modulus $\\mu$ for each supernova. \n",
    "\n",
    "The file `Pantheon+SH0ES.dat` (from Teams or Blackboard) contains the data:\n",
    "- `zHD`: redshift you should use\n",
    "- `MU_SH0ES`: distance modulus\n",
    "\n",
    "The covariance matrix $C$ is provided in `Pantheon+SH0ES_STAT+SYS.cov.txt` (from Teams or Blackboard).\n",
    "It is a square symmetric matrix N x N.\n",
    "The file provided contains a vector (the first row is the size N) which you can just reshape to N x N.\n",
    "\n",
    "The files can also be downloaded here:\n",
    "- https://www.dropbox.com/scl/fi/n67of2kwtmabb2vahk36m/Pantheon-SH0ES.dat?rlkey=mw210zbna7b0pxs4ptj2gdivl&dl=0\n",
    "- https://www.dropbox.com/scl/fi/ncafbjkwh5w83hehs6p0u/Pantheon-SH0ES_STAT-SYS.cov.txt.zip?rlkey=ruxgi74dkz1hcfehxdl4jfns9&dl=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### 1.3. Exercise\n",
    "\n",
    "Write your own HMC code to infer $h$ and $\\Omega_m$ from the supernova dataset, assuming the Universe is flat and the errors are Gaussian.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Nearby supernovae have redshifts significantly altered by 'peculiar velocities', not associated with the general expansion of the Universe. **Discard supernovae with $z < 0.01$.**\n",
    "- $h$ and $\\Omega_m$ are positive, and have values of the rough order of unity\n",
    "- Assume uniform priors on the parameters (so you will sample from the likelihood)\n",
    "- Explore visually (with trace plots) the chain, and compute and report the acceptance rate\n",
    "- Choose a suitable burn-in and say why you chose it\n",
    "- Compute the average value of the parameters under the posterior distribution, and their variances and covariance\n",
    "- Compute and display the correlation function of the chain. Compute the acceptance rate and the effective number of samples, using the formula in the lectures. Explore how these changes as the parameters of the HMC are varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from pathlib import Path\n",
    "\n",
    "# Add your imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "#### Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Loaded 1701 SNe; kept 1590 with z >= 0.01\n",
      "Data columns: ['CID', 'IDSURVEY', 'zHD', 'zHDERR', 'zCMB', 'zCMBERR', 'zHEL', 'zHELERR', 'm_b_corr', 'm_b_corr_err_DIAG', 'MU_SH0ES', 'MU_SH0ES_ERR_DIAG', 'CEPH_DIST', 'IS_CALIBRATOR', 'USED_IN_SH0ES_HF', 'c', 'cERR', 'x1', 'x1ERR', 'mB', 'mBERR', 'x0', 'x0ERR', 'COV_x1_c', 'COV_x1_x0', 'COV_c_x0', 'RA', 'DEC', 'HOST_RA', 'HOST_DEC', 'HOST_ANGSEP', 'VPEC', 'VPECERR', 'MWEBV', 'HOST_LOGMASS', 'HOST_LOGMASS_ERR', 'PKMJD', 'PKMJDERR', 'NDOF', 'FITCHI2', 'FITPROB', 'm_b_corr_err_RAW', 'm_b_corr_err_VPEC', 'biasCor_m_b', 'biasCorErr_m_b', 'biasCor_m_b_COVSCALE', 'biasCor_m_b_COVADD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mihir Koka\\AppData\\Local\\Temp\\ipykernel_700\\3155238785.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(SN_FILE, delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load supernova data\n",
    "# Filter z < 0.01\n",
    "# Your code here\n",
    "\n",
    "DATA_DIR = Path(\".\")\n",
    "SN_FILE = DATA_DIR/\"Pantheon+SH0ES.dat\"\n",
    "COV_FILE = DATA_DIR / \"Pantheon+SH0ES_STAT+SYS.cov.txt\"\n",
    "\n",
    "df = pd.read_csv(SN_FILE, delim_whitespace=True)\n",
    "expected_columns = {\"zHD\", \"MU_SH0ES\"}\n",
    "print(set(df.columns).issuperset(expected_columns))\n",
    "\n",
    "# Discard nearby SNe with z < 0.01, preserve original ordering\n",
    "mask = df[\"zHD\"].to_numpy() >= 0.01\n",
    "idx = np.where(mask)[0]\n",
    "\n",
    "df_f = df.loc[mask].reset_index(drop=True)\n",
    "z_data = df_f[\"zHD\"].to_numpy(dtype=np.float64)\n",
    "mu_data = df_f[\"MU_SH0ES\"].to_numpy(dtype=np.float64)\n",
    "\n",
    "print(f\"Loaded {len(df)} SNe; kept {len(z_data)} with z >= 0.01\")\n",
    "print(f\"Data columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load covariance matrix and extract corresponding subset after filtering\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "#### Implement the theoretical model (Pen 1999 formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta(a, Omega_m):\n",
    "    \"\"\"\n",
    "    Helper function for Pen (1999) formula.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a : float or array\n",
    "        Scale factor (a = 1/(1+z))\n",
    "    Omega_m : float\n",
    "        Matter density parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    eta : float or array\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def luminosity_distance_star(z, Omega_m):\n",
    "    \"\"\"\n",
    "    Calculate D_L^* (luminosity distance with h=1 factored out) using Pen (1999).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : float or array\n",
    "        Redshift\n",
    "    Omega_m : float\n",
    "        Matter density parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    D_L_star : float or array\n",
    "        Luminosity distance in Mpc (with h=1)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def distance_modulus(z, Omega_m, h):\n",
    "    \"\"\"\n",
    "    Calculate theoretical distance modulus.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : float or array\n",
    "        Redshift\n",
    "    Omega_m : float\n",
    "        Matter density parameter\n",
    "    h : float\n",
    "        Reduced Hubble constant\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mu : float or array\n",
    "        Distance modulus\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test these functions! Check that you can get reasonable values given typical parameters and redshifts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "#### Implement the likelihood and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(params, z_data, mu_data, C_inv):\n",
    "    \"\"\"\n",
    "    Calculate log-likelihood for supernova data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : array\n",
    "        [Omega_m, h]\n",
    "    z_data : array\n",
    "        Redshift data\n",
    "    mu_data : array\n",
    "        Distance modulus data\n",
    "    C_inv : array\n",
    "        Inverse covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    log_L : float\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def gradient_log_likelihood(params, z_data, mu_data, C_inv):\n",
    "    \"\"\"\n",
    "    Calculate gradient of log-likelihood with respect to parameters.\n",
    "    \n",
    "    Three options: \n",
    "    - derive the gradient analytically and implement it\n",
    "    - use numerical differentiation (finite differences)\n",
    "    - use automatic differentiation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : array\n",
    "        [Omega_m, h]\n",
    "    z_data : array\n",
    "        Redshift data\n",
    "    mu_data : array\n",
    "        Distance modulus data\n",
    "    C_inv : array\n",
    "        Inverse covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    grad : array\n",
    "        Gradient [d(log L)/d(Omega_m), d(log L)/dh]\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "#### Implement HMC sampler\n",
    "\n",
    "Implement the Hamiltonian Monte Carlo algorithm, including:\n",
    "- Leapfrog integrator for Hamiltonian dynamics\n",
    "- Metropolis acceptance step\n",
    "- Main HMC loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog(q, p, epsilon, L, grad_log_prob, *args):\n",
    "    \"\"\"\n",
    "    Leapfrog integrator for Hamiltonian dynamics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    q : array\n",
    "        Current position (parameters)\n",
    "    p : array\n",
    "        Current momentum\n",
    "    epsilon : float\n",
    "        Step size\n",
    "    L : int\n",
    "        Number of leapfrog steps\n",
    "    grad_log_prob : function\n",
    "        Function to calculate gradient of log probability\n",
    "    *args : additional arguments\n",
    "        Additional arguments to pass to grad_log_prob\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    q_new : array\n",
    "        New position\n",
    "    p_new : array\n",
    "        New momentum\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def hmc_sampler(initial_params, n_samples, epsilon, L, log_prob, grad_log_prob, *args):\n",
    "    \"\"\"\n",
    "    Hamiltonian Monte Carlo sampler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    initial_params : array\n",
    "        Initial parameter values [Omega_m, h]\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    epsilon : float\n",
    "        Step size for leapfrog integrator\n",
    "    L : int\n",
    "        Number of leapfrog steps per iteration\n",
    "    log_prob : function\n",
    "        Log probability function\n",
    "    grad_log_prob : function\n",
    "        Gradient of log probability\n",
    "    *args : additional arguments\n",
    "        Additional arguments to pass to log_prob and grad_log_prob\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array\n",
    "        MCMC samples (n_samples x n_params)\n",
    "    acceptance_rate : float\n",
    "        Fraction of proposals accepted\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "#### Run HMC sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HMC parameters\n",
    "# initial_params = [Omega_m_init, h_init]\n",
    "# n_samples = ...\n",
    "# epsilon = ...\n",
    "# L = ...\n",
    "\n",
    "# Run HMC\n",
    "# samples, acceptance_rate = hmc_sampler(...)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "#### Trace plots and acceptance rate\n",
    "\n",
    "Create trace plots to visualize the MCMC chain and report the acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trace plots\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "#### Burn-in and posterior analysis\n",
    "\n",
    "Choose an appropriate burn-in period and compute posterior statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose burn-in based on trace plots\n",
    "# burn_in = ...\n",
    "\n",
    "# Remove burn-in samples\n",
    "# samples_post_burnin = samples[burn_in:]\n",
    "\n",
    "# Compute posterior mean\n",
    "# mean_Omega_m = ...\n",
    "# mean_h = ...\n",
    "\n",
    "# Compute posterior variance\n",
    "# var_Omega_m = ...\n",
    "# var_h = ...\n",
    "\n",
    "# Compute posterior covariance\n",
    "# cov_matrix = ...\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "#### Autocorrelation function and effective sample size\n",
    "\n",
    "Compute and plot the autocorrelation function, then calculate the effective number of independent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute autocorrelation function\n",
    "# Your code here\n",
    "\n",
    "# Plot autocorrelation\n",
    "# Your code here\n",
    "\n",
    "# Compute effective sample size\n",
    "# Use formula from lectures: N_eff = N / (1 + 2*sum(rho_k))\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "#### Exploration of HMC parameter effects\n",
    "\n",
    "Systematically explore how the step size $\\epsilon$ and number of leapfrog steps $L$ affect:\n",
    "- Acceptance rate\n",
    "- Effective sample size\n",
    "- Computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary epsilon and L systematically\n",
    "# For each combination, run HMC and compute acceptance rate and N_eff\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "#### Visualization of results\n",
    "\n",
    "Create visualizations of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D posterior plot (contours or scatter)\n",
    "# Can use corner.py package: corner.corner(samples_post_burnin, labels=[r'$\\Omega_m$', r'$h$'])\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### 1.4. Optional Extension\n",
    "\n",
    "- Write and apply a Gelman-Rubin convergence test, and deduce roughly how long the chains should be for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Gelman-Rubin convergence test\n",
    "# Run multiple chains from different starting points\n",
    "# Compute R-hat statistic\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

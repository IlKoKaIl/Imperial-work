{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation and Fitting Data\n",
    "\n",
    "In data science, you will often be required to interpolate between your data points or fit your data, i.e. describe the behaviour seen in your data with a mathematical function. This can be because you need to use points that lie between your actual experimental data points, you would like to parameterize your data, or extract some physical value from the data you have taken with associated uncertainty.\n",
    "\n",
    "This notebook serves as an introduction to some of the fitting methods commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Outline: <a id='index'></a>\n",
    "\n",
    "1. [Interpolation](#i)\n",
    "    1. [Linear Interpolation](#li)\n",
    "    1. [Cubic Spline](#cs)\n",
    "1. [Introduction to Data Fitting](#im)\n",
    "1. [Data Fitting Using iminuit](#ui)\n",
    "    1. [The Basics](#tb)\n",
    "    1. [Interlude: Gradient Descent](#gd)\n",
    "    1. [The Minuit Object](#tmo)\n",
    "    1. [Parameter Uncertainties, Covariances, and Confidence Intervals](#pucaci)\n",
    "    1. [Cost Functions](#cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "# Interpolation [^](#index) <a id='i'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way of approximating points between your collected data is by **interpolation**. Compared to a \"full fit\" where you fit all of your data points with a single mathematical function, interpolation approximates between your data piecewise. This means it divides the data points into different subsets and fits these separately, making sure the different fits join togehter at connecting boundaries and depending on the algorithm additionally that they have the same slope and curvature at these points.\n",
    "\n",
    "Two different methods of interpolation are introduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Linear Interpolation [^](#index) <a id='li'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to interpolate between the data taken from an experiment, the simplest method is to draw a straight line between each pair of data points and use this to estimate the points in between. This is called **linear interpolation**. While you can write your own linear interpolation algorithm, a built-in option is available in `scipy`. \n",
    "\n",
    "Below is an example using linear interpolation to approximate data points from a sine, showing both the approximation compared to the function, as well as the distribution of errors, i.e. the discrepancies between interpolated and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "import scipy.interpolate as spi\n",
    "pl.rcParams['figure.figsize'] = [10, 15] \n",
    "\n",
    "\n",
    "x=np.arange(0,6.5,0.5) # plus 0.5 to make sure that we get 6\n",
    "y=np.sin(x)\n",
    "\n",
    "# now get the finer binned version for comparison\n",
    "xf=np.arange(0,6,0.001) \n",
    "yf=np.sin(xf)\n",
    "\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(xf,yf,\"g--\",label=\"True Values\")\n",
    "pl.xlabel(\"$x$\")\n",
    "pl.ylabel(\"$y$\")\n",
    "\n",
    "# now the linear 1D interpololation\n",
    "f=spi.interp1d(x, y)\n",
    "\n",
    "# f is a function that will return you a value of y for any x\n",
    "# so now lets compare the difference at a finer scale\n",
    "pl.plot(xf,f(xf), label=\"Linear Interpolation\")\n",
    "\n",
    "pl.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n",
    "#now lets histogram the differences.\n",
    "ydiff=yf-f(xf)\n",
    "pl.subplot(2,1,2)\n",
    "pl.hist(ydiff,bins=100)\n",
    "pl.xlabel(\"Difference between interpolated and true values\")\n",
    "pl.ylabel(\"Number of enteries\")\n",
    "#pl.xlim([-0.04,0.04])\n",
    "\n",
    "\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown from the plot above, not only can the differences be quite large, but there is also structure in the differences, which can cause a systematic bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Cubic Spline [^](#index) <a id='cs'></a>\n",
    "\n",
    "An alternative to linear interpolation is to use a **spline**, with a **cubic spline** being the most commonly used as it provides smoothness and double differentiability. \n",
    "\n",
    "The example below is identical to the previous one, except that instead of linear interpolation, a cubic spline has been employed. To achieve this, we use the same `spi.interp1d` function, but specify the keyword argument `kind='cubic'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "import scipy.interpolate as spi\n",
    "pl.rcParams['figure.figsize'] = [10, 15] \n",
    "\n",
    "\n",
    "x=np.arange(0,6.5,0.5) # plus 0.5 to make sure that we get 6\n",
    "y=np.sin(x)\n",
    "\n",
    "# now get the finer binned version for comparison\n",
    "xf=np.arange(0,6,0.001) \n",
    "yf=np.sin(xf)\n",
    "\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(xf,yf,\"g--\",label=\"True Values\")\n",
    "pl.xlabel(\"$x$\")\n",
    "pl.ylabel(\"$y$\")\n",
    "\n",
    "# now the cubic spline interpolation. Notice that we must specify the keyword argument kind='cubic'\n",
    "f=spi.interp1d(x, y, kind='cubic')\n",
    "\n",
    "# f is a function that will return you a value of y for any x\n",
    "# so now lets compare the difference at a finer scale\n",
    "\n",
    "pl.plot(xf,f(xf), label=\"Cubic Spline\")\n",
    "\n",
    "pl.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n",
    "# visualise the differences on a histogram\n",
    "\n",
    "\n",
    "ydiff=yf-f(xf)\n",
    "pl.subplot(2,1,2)\n",
    "pl.hist(ydiff,bins=100)\n",
    "pl.xlabel(\"Difference between interpolated and true values\")\n",
    "pl.ylabel(\"Number of enteries\")\n",
    "#pl.xlim([-0.04,0.04])\n",
    "\n",
    "\n",
    "\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using a cubic spline as an approximation provides a much better fit, with significantly smaller differences between the interpolated and true values *(notice that the $x$-axis scale is much smaller than in the linear example)*. However, even with this improved accuracy, there is still a structure present, resulting in systematic biases, even if these biases are a lot smaller due to the smaller differences.\n",
    "\n",
    "SciPy also implements two-dimensional forms of these interpolation algorithms.\n",
    "\n",
    "It's important to note that while using a cubic spline is generally effective, it is not a foolproof solution. In cases where there aren't enough data points, the cubic spline can occasionally overestimate or underestimate the true curve, as shown in the example below. However, it is generally pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pl.rcParams['figure.figsize'] = [10, 10] \n",
    "x = np.linspace(0, 10, num=11, endpoint=True)\n",
    "y = np.cos(-x**2/9.0)\n",
    "f = spi.interp1d(x, y)\n",
    "f2 = spi.interp1d(x, y, kind='cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnew = np.linspace(0, 10, num=101, endpoint=True)\n",
    "import matplotlib.pyplot as pl\n",
    "yt=np.cos(-xnew**2/9.0)\n",
    "pl.plot(x, y, 'o', xnew, f(xnew), '-', xnew, f2(xnew), '--',xnew,yt,'r:')\n",
    "pl.legend(['data', 'linear', 'cubic','true'], loc='best')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise ##\n",
    "\n",
    "The purpose of this exercise is to understand how interpolations work and their limitations.\n",
    "\n",
    "Select a function that has different scales of structure (such as $y=\\cos(x^2)$ with $0 \\le x \\lt 10$ for example) and investigate how well a cubic spline is able to interpolate with different sampling separations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Introduction to Data Fitting [^](#index) <a id='im'></a>\n",
    "\n",
    "One of the most important elements in data analysis is often fitting the data distributions. In performing a fit you minimise how far your fitted solution deviates from the data, whilst making some assumptions about the data. You may be assuming that they are distributed according to a well defined probability density function (pdf), or that the data points should follow some functional form. In the measurement of all physical quantities, the **uncertainty** on that measurement is paramount to your true understanding of the measurement. \n",
    "\n",
    "SciPy does have a series of functions for fitting data in `scipy.optimize` and these are pretty good. However, we tend to use the more advanced [minuit](https://root.cern.ch/download/minuit.pdf) developed by CERN. This is written in C++ and wrapped in python as [iminuit](https://iminuit.readthedocs.io/en/stable/). \n",
    "\n",
    "`iminuit` has some fantastic [tutorials](https://iminuit.readthedocs.io/en/stable/tutorials.html) on their website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Data Fitting Using iminuit  [^](#index) <a id='ui'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will introduce the basics of data fitting, how to approach standard fitting problems and how to do all this using the iminuit package. At the end of this notebook is a **long** exercise designed to give you more practice fitting a curve from the ground up. \n",
    "\n",
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## The Basics [^](#index) <a id='tb'></a>\n",
    "\n",
    "`iminuit` is a Python frontend to the Minuit library in C++, an integrated software that combines a local minimizer and two error calculators, called Hesse and Minos. The local minimizer is called Migrad and uses the method of gradient descent and Newton's method. You provide it an analytical function, which accepts one or several parameters, and an initial guess of the parameter values. It will then find a local minimum of this function starting from the initial guess. In that regard, the iminuit minimizer is just like other local minimizers, e.g. those in scipy.optimize.\n",
    "\n",
    "In addition, iminuit has the ability to compute uncertainty estimates for model parameters. iminuit was designed to solve statistics problems, where uncertainty estimates are an essential part of the result. The two ways of computing uncertainty estimates, Hesse and Minos, have different advantages and disadvantages.\n",
    "\n",
    "iminuit is the successor of pyminuit. If you used pyminuit before, you will find iminuit very familiar. An important feature of iminuit (and pyminuit) is that it uses introspection to detect the parameter names of your function. This is very convenient, especially when you work interactively in a Jupyter notebook. It also provides special output routines for Jupyter notebooks to print the fit results in a nice way, as you will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setup of the notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# everything in iminuit is done through the Minuit object, so we import it\n",
    "from iminuit import Minuit\n",
    "\n",
    "# we also need a cost function to fit and import the LeastSquares function\n",
    "from iminuit.cost import LeastSquares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "<div style=\"background-color: #FFFACD; padding: 10px;\">\n",
    "\n",
    "## Interlude: Gradient Descent [^](#index) <a id='gd'></a>\n",
    "\n",
    "**Note: This is taken from the statistics course** *and you will cover this in a few weeks there. So if it doesn't make sense yet, there's no need to worry.*\n",
    "    \n",
    "For functions with more than one variable, we can use a gradient descent algorithm to find minima (or maxima). These methods only rely on the first derivative (the gradient) to find the best direction to step in to find the minimum (or maximum). They don’t use the second derivative because for a large number of parameters $n$, the number of terms in the second derivative grows as $n^2$ so this can get quite costly.\n",
    "    \n",
    "There are a number of subtly different algorithms which rely on gradient descent but we’ll go over\n",
    "a simple example. The way this works is as follows;\n",
    "\n",
    "First, we initialise the algorithm at some set of values for the parameters $\\boldsymbol{\\theta}_{init}$. Then we calcualte the gradient of what you are trying to minimise - often the negative log-likelihood - $\\nabla(q)\\rvert_{\\boldsymbol{\\theta}_{init}}$ - at that initial point. The gradient defines a direction in which the negative log-likelihood changes most rapidly - this makes this direction a good direction to search for a smaller value of $q$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla(q)\\rvert_{\\boldsymbol{\\theta}_{init}} &=   \n",
    "        \\begin{bmatrix}\n",
    "           \\frac{\\partial q}{\\partial \\theta_{1}} \\\\\n",
    "           \\frac{\\partial q}{\\partial \\theta_{2}} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\frac{\\partial q}{\\partial \\theta_{n}}\n",
    "        \\end{bmatrix}_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_{init}}\n",
    "\\end{align}   \n",
    "$$\n",
    "\n",
    "Since we want to *decrease* $q$, we need to move against this gradient. We step along the negative gradient direction until we find a new minimum - i.e. we pick a step sise $h$ and keep adding it to the initial point until the value of $q$ stops decreasing, \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{k} = k\\times h (- \\nabla(q)\\rvert_{\\boldsymbol{\\theta}_{init}}) \n",
    "$$\n",
    "\n",
    "At the value of $k$ such that we’re no longer decreasing $q$ (lets call it $k^∗$), we recalculate the gradient\n",
    "and find a new direction to go in.\n",
    "    \n",
    "$$\n",
    "\\nabla(q)\\rvert_{\\boldsymbol{\\theta}_{k^*}}\n",
    "$$\n",
    "    \n",
    "We keep repeating this process until the modulus of the gradient gets close enough to zero (say is\n",
    "smaller than some tolerance $\\epsilon$), that means we keep iterating until,\n",
    "    \n",
    "$$\n",
    "|\\nabla(q)\\rvert_{\\boldsymbol{\\theta}}| = \\sqrt{\\left(\\frac{\\partial q}{\\partial \\theta_{1}}\\right) + \\left(\\frac{\\partial q}{\\partial \\theta_{2}}\\right) + ... + \\left(\\frac{\\partial q}{\\partial \\theta_{n}}\\right)} \\lt \\epsilon \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## The Basics - Continued  [^](#index)\n",
    "\n",
    "### Quick start\n",
    "\n",
    "In this first section, we look at a simple case where a straight line should be fitted to a scattered ($x$,$y$) dataset. A line has two parameters, which we call (α,β). We go through the full fit, showing all basic steps to get you started quickly. In subsequent sections we will revisit the steps in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our line model, unicode parameter names are supported :)\n",
    "def line(x, α, β):\n",
    "    return  α + β * x\n",
    "\n",
    "\n",
    "# generate random toy data with random offsets in y\n",
    "np.random.seed(1)\n",
    "data_x = np.linspace(0, 1, 10)\n",
    "data_yerr = 0.1  # could also be an array\n",
    "data_y = line(data_x, 1, 2) + data_yerr * np.random.randn(len(data_x))\n",
    "\n",
    "# draw toy data\n",
    "plt.errorbar(data_x, data_y, data_yerr, fmt=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*Pro-tip:* If you want to use Greek letters, consider installing an additional Greek keyboard layout for easier typing.)\n",
    "\n",
    "To recover the parameters α and β of the line model from this data, we need to minimize a suitable cost function. The cost function must be twice differentiable and have a minimum at the optimal parameters. We use the method of least-squares here, whose cost function computes the sum of squared residuals between the model and the data. The task of iminuit is to find the minimum of that function. Let’s do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the line below is purely for formatting our plots\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10] \n",
    "\n",
    "# iminuit contains a LeastSquares class to conveniently generate a least-squares cost function.\n",
    "# We will revisit how to write this by hand in a later section.\n",
    "\n",
    "least_squares = LeastSquares(data_x, data_y, data_yerr, line)\n",
    "\n",
    "m = Minuit(least_squares, α=0, β=0)  # starting values for α and β.\n",
    "\n",
    "m.migrad()  # finds minimum of least_squares function\n",
    "m.hesse()   # accurately computes uncertainties\n",
    "\n",
    "\n",
    "\n",
    "#print the values and errors and then note how to access individual ones.\n",
    "\n",
    "print(m)\n",
    "\n",
    "print(m.values)\n",
    "print(m.errors)\n",
    "\n",
    "\n",
    "print(m.values[0])\n",
    "print(m.errors[0])\n",
    "\n",
    "# draw data and fitted line\n",
    "\n",
    "plt.errorbar(data_x, data_y, data_yerr, fmt=\"o\", label=\"data\")\n",
    "plt.plot(data_x, line(data_x, *m.values), label=\"fit\")\n",
    "\n",
    "# display legend with some fit info\n",
    "fit_info = [\n",
    "    f\"$\\\\chi^2$ / $n_\\\\mathrm{{dof}}$ = {m.fval:.1f} / {len(data_x) - m.nfit}\",\n",
    "]\n",
    "\n",
    "# try printing out the parameters, values and errors to see the format\n",
    "\n",
    "for p, v, e in zip(m.parameters, m.values, m.errors):\n",
    "    fit_info.append(f\"{p} = ${v:.3f} \\\\pm {e:.3f}$\")\n",
    "    \n",
    "\n",
    "plt.legend(title=\"\\n\".join(fit_info))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Quick Exercise\n",
    "\n",
    "Why must the cost function be twice differentiable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is already it for a basic fit. Easy, right?\n",
    "\n",
    "In the following, we dive into the details step by step: how the Minuit object is initialized, how to run the algorithms, and how to get the results.\n",
    "\n",
    "iminuit was designed to make it easy to fit functions like `least_squares(...)`, where the parameters are individual arguments of the function. There is an alternative function signature that Minuit supports, which is more convenient when you work a lot with numpy. There, the parameters are passed as a numpy array. The two kinds of function definitions have each their pros and cons. \n",
    "\n",
    "We will first look at how to work with functions of the first kind and come back to the second kind later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## The Minuit Object [^](#index) <a id='tmo'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Minuit object\n",
    "To minimize a function, one has to create an instance of the Minuit class, pass the function, and a starting value for each parameter. To start the minimization then requires an extra step, which we will cover a bit later. For now, let's just focus on the initialization.\n",
    "\n",
    "The Minuit object uses introspection to get the number and names of the function parameters automatically, so that they can be initialized with keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the Minuit class\n",
    "m = Minuit(least_squares, α=0, β=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we forget a parameter or mistype, Minuit will raise an error - see 2 examples of this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Minuit(least_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Minuit(least_squares,a=0,b=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameter values\n",
    "\n",
    "Minuit’s main algorithm Migrad is a local minimizer. It searches for a local minimum by doing a mix of Newton steps and gradient-descents from a starting point. (*Note:* Don't worry if you don't know both methods yet, they will be covered in detail in the statistics course.) If your function has several minima, the minimum found will depend on the starting point. Therefore, you have to be careful with both, choosing your starting point and making sure you found the right minimum after your fit. Because you don't want to end up in a local minimum if you are really interested in the global minimum. Even if your function has only one minimum, if you start in the proximity of the minimum, iminuit will converge to it faster.\n",
    "\n",
    "You can set the starting point using the parameter names of your function as keywords, `name = value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(least_squares, α=5, β=5)  # pass starting values for α and β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the starting values can also be passed as positional arguments, like below:\n",
    "```\n",
    "Minuit(least_squares, 5, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using iminuit with Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use iminuit with functions that accept numpy arrays. This has pros and cons.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Easy to change number of fitted parameters\n",
    "* Sometimes simpler function body that’s easier to read\n",
    "* Technically this is more efficient, but this is hardly going to be noticable\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* iminuit cannot figure out names for each parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we will use a version of the line model which accepts the parameters as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_np(x, par):\n",
    "    return np.polyval(par, x)  # for len(par) == 2, this is a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `line_np` with a different number of arguments is easy. For $n$ arguments, a polynomial of order $n$ is used to predict the behavior of the data.\n",
    "\n",
    "The built-in cost functions support such a model. For it to be detected properly, you need to pass the starting values in the form of a single sequence of numbers.\n",
    "\n",
    "Remember: by starting values, we mean the point at which the minimizer starts from when searching for a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares_np = LeastSquares(data_x, data_y, data_yerr, line_np)\n",
    "\n",
    "Minuit(least_squares_np, (5, 5))  # pass starting values as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a tuple here for initialisation, but any sequence will work. We could instead have passed a list or a numpy array here. `iminuit` uses the length of the sequence to detect how many parameters the model has. By default, the parameters are named automatically $x_0$ to $x_N$. \n",
    "\n",
    "One can override this with the keyword argument `name` and passing a sequence of parameter names. Of course, this sequence must be of the same length as the sequence of starting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(least_squares_np, (5, 5), name=(\"a\", \"b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `least_squares_np` works for parameter arrays of any length, one can easily change the number of fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a forth order polynomial\n",
    "Minuit(least_squares_np, (5, 5, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to try different orders of a polynomial model. If the order is too small, the polynomial will not follow the data. If it is too large, it will overfit the data and pick up random fluctuations and not the underlying trend. \n",
    "\n",
    "We can figure out the right order by experimenting or using an algorithm like cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting current parameters\n",
    "You can check the current parameter values and settings with the method `Minuit.params` at any time. It returns a special list of Param objects which returns nice print outputs in Jupyter and in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a nice table with numbers rounded according to the rules of the Particle Data Group. The table will be updated once you run the actual minimization. To look at the initial conditions later, use `Minuit.init_params`. Don't worry about the values and meaning of Hesse and Minos errors, we will come back to them later.\n",
    "\n",
    "`Minuit.params` returns a tuple-like container of Param objects, which are data objects with attributes that one can query. Use `repr()` to get a detailed representation of the data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in m.params:\n",
    "    print(repr(p), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the fit with iminuit\n",
    "\n",
    "Now that we have learned the initialization basics for a Minuit object, let's see how we can run the actual minimizer before going back to some more advanced initialization topics.\n",
    "\n",
    "To run the fit, we just have to call the (specific) minimizer method of the object. For example, if you want to run Migrad (but you could also do this for other predefined minimizers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.migrad()  #Note that we last initialised the Minuit instance m as a first order polynomial, so it will fit a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we have to do and the minimzer works his magic. We will learn more about this later. But for now, let's see whether it actually did something by inspecting the updated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Fit the following data set with different orders of polynomials. Describe qualitatively how the fit changes and how well the fit describes the data as you increase the order of the polynomial? When does the fit show signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "y_measured = [343.39452514, 251.31914561, 186.7806368, 121.30027965, 45.23336652, 23.49470302, 18.46581766, 6.58329486, 1.98522328, -\n",
    "              6.74799454, -7.55489379, -29.55544088, -31.32898172, -36.27525348, -25.23860169, -1.09156819, 52.32898397, 225.38126087, 460.67437131]\n",
    "\n",
    "y_err = [141.312, 86.205, 49.632, 26.877, 13.872, 7.197, 4.08, 2.397, 0.672,\n",
    "         1.923, 5.568, 9.795, 13.488, 14.883, 11.568, 0.483, 22.08, 60.477, 119.712]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters with limits\n",
    "\n",
    "`iminuit` allows you to set parameter limits. Often a parameter is limited mathematically or physically to a certain range, so you want to include these in your Minuit object before running the fit. For example, if your function contains `sqrt(x)`, then $x$ must be non-negative, i.e. $x \\geq 0$. You can set upper-, lower-, or two-sided limits for each parameter individually with the limits property:\n",
    "\n",
    "* lower limit: use `Minuit.limits = (<value>, None)` or `(<value>, float(\"infinity\"))`\n",
    "* upper limit: use `Minuit.limits = (None, <value>)` or `(-float(\"infinity\"), <value>)`\n",
    "* two-sided limit: use `Minuit.limits = (<min_value>, <max_value>)`\n",
    "* remove limits: use `Minuit.limits = None` or `(-float(\"infinity\"), float(\"infinity\")`\n",
    "\n",
    "    \n",
    " You can also set limits for several parameters at once with a sequence. To impose the limits $\\alpha \\geq 0$ and $0 \\leq \\beta \\leq 10$ in our example, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(least_squares, α=5, β=5)  #we have to initialize it again to forget about our fit\n",
    "\n",
    "m.limits = [(0, None), (0, 10)]\n",
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing and releasing parameters\n",
    "In some cases, you may have a parameter that needs to be temporarily set to a fixed value. This can be useful when you have a guess for its value and want to observe how the other parameters adapt when this specific parameter is held constant at that value.\n",
    "\n",
    "Alternatively, if you have a complex function with numerous parameters that have varying impacts on the function, you can assist the minimizer in finding the minimum faster by initially fixing the less influential parameters to their initial guesses and only fitting the significant parameters. \n",
    "\n",
    "Once the minimum is obtained under these conditions, you can release the fixed parameters and optimize all the parameters together. Minuit retains the previous minimization state and resumes from there. The time required for minimization approximately scales with the square of the number of parameters, $n^2$. By performing iterated minimization over subspaces of the parameters, this time can be reduced.\n",
    "\n",
    "To fix an individual parameter, you can utilise the keyword `Minuit.fixed[name] = True`. In our example, we fix parameter \"α\" as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fixed[\"α\"] = True\n",
    "m.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# migrad will not vary α, only β\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we release α and fix β and minimize again, can also use parameter index\n",
    "m.fixed[0] = False\n",
    "m.fixed[1] = True\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Starting Points for Minimization ###\n",
    "\n",
    "It is sometimes useful to manually change the values of some fixed parameters and fit the others, or to restart the fit from another starting point. \n",
    "\n",
    "For example, if the cost function has several minima, changing the starting value can be used to find the other minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show an example below for a function which has 2 minima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_with_two_minima(x):\n",
    "    return x ** 4 - x ** 2 + 1\n",
    "\n",
    "# we come back to the meaning of errordef in the next section\n",
    "cost_function_with_two_minima.errordef = Minuit.LEAST_SQUARES\n",
    "\n",
    "x = np.linspace(-1.5, 1.5)\n",
    "plt.plot(x, cost_function_with_two_minima(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting at -0.1 gives the left minimum\n",
    "m = Minuit(cost_function_with_two_minima, x=-0.1)\n",
    "m.migrad()\n",
    "print(\"starting value -0.1, minimum at\", m.values[\"x\"])\n",
    "\n",
    "# changing the starting value to 0.1 gives the right minimum\n",
    "m.values[\"x\"] = 0.1  # m.values[0] = 0.1 also works\n",
    "m.migrad()\n",
    "print(\"starting value +0.1, minimum at\", m.values[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "### Advanced: Simplex and Scan minimizers\n",
    "\n",
    "`iminuit` also offers two other minimizers which are less powerful than Migrad, but may be useful in special cases.\n",
    "\n",
    "#### Simplex\n",
    "\n",
    "The Nelder-Mead method (aka SIMPLEX) is well described on [Wikipedia](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method). It is a gradient-free minimization method that usually converges more slowly, but may be more robust. For some problems it can help to start the minimization with SIMPLEX and then finish with MIGRAD. Since the default stopping criterion for SIMPLEX is much more lax than MIGRAD, either running MIGRAD after SIMPLEX or reducing the tolerance with `Minuit.tol` is strongly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(cost_function_with_two_minima, x=10).simplex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "Let’s run MIGRAD to finish the minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(cost_function_with_two_minima, x=10).simplex().migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "This combination uses slightly fewer function evaluations and produces a more accurate result than just running MIGRAD alone in this case, but for another problem this may not be true. \n",
    "    \n",
    "#### Scan\n",
    "\n",
    "Scan is a last resort. It creates a N-dimensional grid scan over the parameter space. The number of function evaluations scale like $nk$, where $k$ is the number of parameters and $n$ the number of steps along one dimension. Using `scan` for high-dimensional problems is unfeasible, but it can be useful in low-dimensional problems and when all but a few parameters are fixed. To run `scan` bounds need to be set, which is best done with `Minuit.limits`. The number of scan points is set with the `ncall` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(cost_function_with_two_minima, x=10)\n",
    "m.limits = (-10, 10)\n",
    "m.scan(ncall=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "The scan brought us in close proximity of the minimum.\n",
    "\n",
    "In this case, the minimum is considered valid, because the **EDM** (**estimated distance to minimum**) value is smaller than the EDM goal, but the scan may also end up in an invalid minimum, which is also ok. The scan minimizes the cost function using a finite number of steps, regardless of the EDM value, which is only computed after the scan for the minimum has finished.\n",
    "\n",
    "One should always run MIGRAD or SIMPLEX after a SCAN. This can often be a good way to find the minimum that you are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "### Advanced: Errordef\n",
    "\n",
    "If you do not use one of the cost functions from the `iminuit.cost` module, you need to pass an additional parameter to Minuit. Let’s make a custom least-squares function and try to run Migrad on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple least-squares cost function looks like this...\n",
    "def custom_least_squares(a, b):\n",
    "    ym = line(data_x, a, b)\n",
    "    z = (data_y - ym) / data_yerr\n",
    "    return np.sum(z ** 2)\n",
    "\n",
    "\n",
    "Minuit(custom_least_squares, a=5, b=5).migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "Minuit now warns about using its default value for the `errordef` parameter, which may not be appropriate. Setting this is not needed for the cost functions in `iminuit.cost`, but it is needed for custom cost functions.\n",
    "\n",
    "The `errordef` parameter is used to compute the correct uncertainties. If you don’t care about uncertainty estimates, you can ignore the warning. In statistical problems, there are two kinds of cost functions to minimize, the negative log-likelihood and the least-squares function, and you will learn more about them in your statistics course.\n",
    "\n",
    "Each has a corresponding value for `errordef`:\n",
    "- -0.5 or the constant: `Minuit.LIKELIHOOD` for negative log-likelihood functions\n",
    "- -1 or the constant: `Minuit.LEAST_SQUARES` for least-squares functions\n",
    "\n",
    "The origin of these numbers is not too complicated, but cannot be explained briefly. If you are curious, have a look into  [“Error computation with HESSE and MINOS”](https://iminuit.readthedocs.io/en/stable/notebooks/hesse_and_minos.html), which explains in depth how uncertainties are computed and where this value comes from.\n",
    "\n",
    "For our custom cost function, we need to set `m.errordef=1` or equivalent and more readable `m.errordef=Minuit.LEAST_SQUARES`, because it is of the least-squares type. If we do that, the warning disappears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(custom_least_squares, a=5, b=5)\n",
    "m.errordef = Minuit.LEAST_SQUARES\n",
    "m.migrad()  # no warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the fit status\n",
    "\n",
    "As seen before, calling `Minuit.migrad()` runs the actual minimization with the Migrad algorithm. Migrad essentially tries a Newton-step and if that does not produce a smaller function value, it tries a line search along the direction of the gradient. So far so ordinary. The clever bits in Migrad are how various pathological cases are handled.\n",
    "\n",
    "Let’s look again at the output of `Minuit.migrad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(least_squares, α=5, β=5)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Minuit.migrad()` method returns the Minuit instance so that one can chain method calls, the instance also prints the latest state of the minimization in a nice way.\n",
    "\n",
    "The *first block* in this output is showing information about the function minimum. This is good for a quick check:\n",
    "\n",
    "All blocks should be green, other colours mean:\n",
    "* Purple means something bad.\n",
    "* Yellow may be bad or not. Be careful.\n",
    "\n",
    "Let’s see how it looks when the function is bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_bad = Minuit(lambda x: 0, x=1)  # a constant function has no minimum\n",
    "m_bad.errordef = 1  # avoid the errordef warning\n",
    "m_bad.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to our previous good example, the info about the function minimum can be directly accessed with `Minuit.fmin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(repr(...)) to see a detailed representation of the data object\n",
    "print(repr(m.fmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important one here is `is_valid`. If this is false, the fit does not converge and the result is useless. Since this value is so often looked at, a shortcut is provided with `Minuit.valid`.\n",
    "\n",
    "If the fit fails, there is usually a numerical or logical issue.\n",
    "\n",
    "Either:\n",
    "\n",
    "- The fit function is not analytical everywhere in the parameter space, e.g. it has a discrete step;\n",
    "\n",
    "or\n",
    "\n",
    "- The fit function does not have a local minimum, e.g. the minimum may be at infinity or the extremum may be a saddle point or maximum.\n",
    "\n",
    "Indicators for this are `is_above_max_edm=True`, `hesse_failed=True`, `has_posdef_covar=False`, or `has_made_posdef_covar=True`.\n",
    "\n",
    "Possible problems are:\n",
    "\n",
    "\n",
    "* Migrad reached the call limit before the convergence so that `has_reached_call_limit=True`. The used number of function calls is `nfcn`, and the call limit can be changed with the keyword argument `ncall` in the method `Minuit.migrad()`. Note that `nfcn` can be slightly larger than `ncall`, because Migrad internally only checks this condition after a full iteration, in which several function calls can happen.\n",
    "* Migrad detects convergence by a small `edm` value, the estimated distance to minimum. This is the difference between the current minimum value of the minimized function and the prediction based on the current local quadratic approximation of the function, which is something that Migrad computes as part of its algorithm. If the fit does not converge, `is_above_max_edm` is true.\n",
    "\n",
    "If you are interested in parameter uncertainties, you should make sure that:\n",
    "\n",
    "* `has_covariance`, `has_accurate_covar`, and `has_posdef_covar` are `True`; and\n",
    "* `has_made_posdef_covar` and `hesse_failed` are `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The *second block* that is printed with `Minuit.migrad()` after information about the fit minimum is the parameter list, which can also be directly accessed with `Minuit.params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m.params` is a tuple-like container of Param data objects which contain information about the fitted parameters. Important fields are: \n",
    "- `number`: parameter index. \n",
    "- `name`: parameter name. \n",
    "- `value`: value of the parameter at the minimum. \n",
    "- `error`: uncertainty estimate for the parameter value.\n",
    "\n",
    "The accuracy of the uncertainty estimate depends on two factors: the correct mathematical modeling of the fitting problem and the appropriate usage of the `errordef` value in Minuit. But what exactly do we mean by \"correct mathematical modeling\"? To understand this, let's examine our simple cost function `custom_least_squares(a, b)`. \n",
    "\n",
    "Notice that each squared residual, i.e. `(data_y - ym)**2`, is divided by the expected variance of the residual, `data_yerr**2`. This division is crucial for obtaining accurate uncertainty estimates for the parameters.\n",
    "\n",
    "In some cases, the expected variance of the residual may not be well-known. When the function to minimize is a least-squares function, there is a simple test to assess the adequacy of the residual variances. One can evaluate the function value at the minimum, denoted by `fmin.fval`, and divide it by the difference between the number of residuals and the number of fitted parameters. This difference can be conveniently obtained using the `nfit` attribute. This metric is referred to as the reduced $\\chi^2$. You become more familiar with this in your statistics course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fval / (len(data_y) - m.nfit)  # reduced chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value should be around 1. The more data points one has, the closer. If the value is much larger than 1, then the data variance is underestimated or the model does not describe the data. If the value is much smaller than 1, then the data variance is overestimated, perhaps because of positive correlations between the fluctuations of the data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The *last block* that gets printed by running `Minuit.migrad()` shows the covariance matrix, which can also be directly accessed with `Minuit.covariance`. This is useful to check for large correlations which are usually a sign of trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise Continued\n",
    "\n",
    "Now that we have introduced how to investigate the fit status, go back to your polynomial fits from before. As you increase the order of the polynomial how does the goodness of fit change? Can you now tell by some goodness of fit measure when the fit shows signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Parameter Uncertainties, Covariances, and Confidence Intervals [^](#index) <a id='pucaci'></a>\n",
    "\n",
    "You saw how to get the uncertainty of each individual parameter and how to access the full covariance matrix of all parameters together, which includes the correlations. Correlations are essential additional information if you want to work with parameter uncertainties seriously.\n",
    "\n",
    "Minuit offers two ways to compute the parameter uncertainties, Hesse and Minos. Both have pros and cons.\n",
    "\n",
    "### Hesse for Covariance and Correlation Matrices\n",
    "The Hesse algorithm numerically computes the matrix of second derivatives at the function minimum (called the Hesse matrix) and inverts it. The Hesse matrix is symmetric by construction. In the limit of infinite data samples used in the fit, the result of this computation converges to the true covariance matrix of the parameters. It is often already a good approximation, even for finite statistic. The errors obtained from this method are sometimes called parabolic errors, because the Hesse matrix method is exact if the function is a hyperparabola, i.e. third and higher-order derivatives are all zero.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* (Comparably) fast computation.\n",
    "* Provides covariance matrix for error propagation.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* May not have good coverage probability when sample size is small\n",
    "\n",
    "The Migrad algorithm computes an approximation of the Hesse matrix automatically during minimization. When the default strategy is used, Minuit does a check whether this approximation is sufficiently accurate and if not, it computes the Hesse matrix automatically.\n",
    "\n",
    "All this happens inside C++ Minuit and is a bit intransparent, so to be on the safe side, we recommend to call `Minuit.hesse` explicitly after the minimization, if the exact uncertainties are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The Hesse matrix (or Hessian) is a square matrix of second-order partial derivatives of a scalar-valued function.\n",
    "The notation for the Hesse Matrix of a function $f$ is $H_f$. The value in the $i$-th row and $j$-th column is given by: $$(H_f)_{i,j} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's mess up the current errors a bit so that hesse has something to do\n",
    "m.errors = (0.16, 0.2)\n",
    "m.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.hesse().params # note the change in \"Hesse Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation Matrices\n",
    "\n",
    "As mentioned before, to see the covariance matrix of the parameters, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(m.covariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in order to get the correlation matrix, you use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance.correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonzero correlation is not necessarily a bad thing, but if you have freedom in redefining the parameters of the fit function, it is good to choose parameters which aren't strongly correlated.\n",
    "\n",
    "Minuit cannot accurately minimise the function if two parameters are (almost) perfectly (anti-)correlated. It also means that one of two parameters is unneeded - it doesn’t add new information. You should rethink the fit function in this case and try to remove one of the parameters from the fit.\n",
    "\n",
    "Both matrices are subclasses of `numpy.ndarray`, so you can use them everywhere you would use a numpy array. In addition, these matrices support value access via parameter names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance[\"α\", \"β\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minos for non-parabolic minima\n",
    "Minuit has another algorithm to compute uncertainties: Minos. It implements the so-called profile likelihood method, where the neighbourhood around the function minimum is scanned until the contour is found where the function increases by the value of `errordef`. The contour defines a confidence region that covers the true parameter point with a certain probability. This probability is exactly known in the limit of infinitely large data samples, but approximate for the finite case. You will learn more about the profile likelihood method in your statistics course, but you can also consult a textbook about statistics about the mathematical details or look at the tutorial “Error computation with HESSE and MINOS”.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Produces confidence regions in 2D (or higher) for scientific plots\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Computationally expensive\n",
    "* It is difficult to propagate uncertainties for asymmetric errors\n",
    "\n",
    "Minos is not automatically called during minimization, it needs to be called explicitly afterwards, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.minos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you have likely become accustomed to seeing green-colored boxes, which indicate that Minos has run successfully. However, it is crucial to exercise caution if these boxes turn red instead, as it signifies a failure of Minos. The fields in the new Minos table carry the following meanings:\n",
    "\n",
    "* `Valid`: Indicates whether Minos considers the scan result valid. \n",
    "* `At Limit`: Becomes true if Minos encounters a parameter limit before completing the contour, which is undesirable.\n",
    "* `Max FCN`: Becomes true if Minos reaches the maximum number of allowed calls before completing the contour, also an undesirable outcome.\n",
    "* `New Min`: Becomes true if Minos discovers a deeper local minimum in the vicinity of the current one. While not necessarily problematic, it should ideally be avoided.\n",
    "\n",
    "The errors computed by Minos are now also shown in the parameter list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Probability of Intervals Constructed with Hesse and Minos Algorithms\n",
    "\n",
    "In applications, it is important to construct confidence regions with a well-known coverage probability. As previously mentioned, the coverage probability of the intervals constructed from the uncertainties reported by Hesse and Minos are not necessarily the standard 68%.\n",
    "\n",
    "Whether Hesse or Minos produce an interval with a coverage probability closer to the desired 68% in finite samples depends on the case. There are theoretical results which suggest that Hesse may be slightly better, but we also found special cases where Minos intervals performed better.\n",
    "\n",
    "Some sources claim that Minos gives better coverage when the cost function is not parabolic around the minimum; that is not generally true, in fact Hesse intervals may have better coverage.\n",
    "\n",
    "As a rule-of-thumb, use Hesse as the default and try both algorithms if accurate coverage probability matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick access to fit results\n",
    "You get the main fit results with properties and methods from the Minuit object. We used several of them already. Here is a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.values)  # array-like view of the parameter values\n",
    "\n",
    "# access values by name or index\n",
    "print(\"by name \", m.values[\"α\"])\n",
    "print(\"by index\", m.values[0])\n",
    "\n",
    "# iterate over values\n",
    "for key, value in zip(m.parameters, m.values):\n",
    "    print(f\"{key} = {value}\")\n",
    "    \n",
    "# slicing works\n",
    "print(m.values[:1])\n",
    "\n",
    "print(m.errors)  # array-like view of symmetric uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Minuit.errors` supports the same access as `Minuit.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.params) # parameter info (using str(m.params))\n",
    "\n",
    "print(repr(m.params)) # parameter info (using repr(m.params))\n",
    "\n",
    "# asymmetric uncertainties (using str(m.merrors))\n",
    "print(m.merrors)\n",
    "\n",
    "print(m.covariance)  # covariance matrix computed by Hesse (using str(m.covariance))\n",
    "print(repr(m.covariance))  # covariance matrix computed by Hesse (using repr(m.covariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, you can play around with iminuit by assigning new values to `m.values` and `m.errors` and then run `m.migrad()` again. The new values will be used as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "`iminuit` comes with built-in methods to draw the confidence regions around the minimum, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum again after messing around with the parameters\n",
    "m.migrad()\n",
    "\n",
    "# draw three contours with 68%, 90%, 99% confidence level\n",
    "m.draw_mncontour(\"α\", \"β\", cl=(0.68, 0.9, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asymptotically, i.e. in large samples, the `cl` is equal to the probability that the region contains the true value. In finite samples, this is usually only approximately the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual contours to plot them yourself\n",
    "ctr_xy = m.mncontour(\"α\", \"β\", cl=0.68, size=10)\n",
    "print(ctr_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to inspect the cost function around the minimum because Minuit warns you about some issues, you can quickly scan it with a call to `Minuit.draw_profile()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.draw_profile(\"α\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use this to plot the result of the scan yourself\n",
    "a, fa = m.profile(\"α\")\n",
    "plt.plot(a, fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `Minuit.mnprofile()` to do a full profile likelihood scan. This mimics what MINOS does to compute confidence intervals. If you have trouble with MINOS, running this may help to inspect the issue.\n",
    "\n",
    "However, this is computationally expensive, since the scan runs MIGRAD for each point on the profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.draw_mnprofile(\"α\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also look at the 2D contours of the cost function around the minimum. Note that these are just contours of the fit function, not confidence regions. The latter you can only get from `Minuit.mncontour()` as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = m.contour(\"α\", \"β\", subtract_min=True)\n",
    "#print(len(x))\n",
    "cs = plt.contour(x, y, z, (1, 2, 3, 4))  # these are not sigmas, just the contour values\n",
    "plt.clabel(cs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use this function for a quick look\n",
    "m.draw_contour(\"α\", \"β\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Cost Functions [^](#index) <a id='cost'></a>\n",
    "\n",
    "The iminuit package comes with a couple of common cost functions that you can import from `iminuit.cost` for convenience. Of course, you can write your own cost functions to use with iminuit, but most of the cost function is always the same. What really varies is the statistical model which predicts the probability density as a function of the parameter values. This is the part that you still have to provide yourself and the iminuit package will not include machinery to build statistical models, as that is out of scope of the package.\n",
    "\n",
    "Using the built-in cost functions is not only convenient, they also have some extra features.\n",
    "\n",
    "* Support of fitted weighted histograms.\n",
    "* Technical tricks to improve numerical stability.\n",
    "* Optional `numba` acceleration (if `numba` is installed).\n",
    "* Cost functions can be added to fit data sets with shared parameters.\n",
    "* Temporarily mask data.\n",
    "\n",
    "We demonstrate each cost function on a standard example from high-energy physics, the fit of a peak over some smooth background. Here, the background is taken to be constant.\n",
    "\n",
    "Let's start by importing the modules that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import cost, Minuit\n",
    "from scipy.stats import norm, uniform\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate our data. We sample from a Gaussian peak around 0 with width 0.1 and from a uniform background between -1 and 1. We then bin the original data. One can fit the original or the binned data. The latter is often much faster and if the binning is fine enough, there is no loss in precision as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = -1, 1\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "xdata = rng.normal(0, 0.1, size=400)\n",
    "xdata = np.append(xdata, rng.uniform(*xrange, size=1000))\n",
    "\n",
    "n, xe = np.histogram(xdata, bins=50, range=xrange)  \n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "\n",
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.plot(xdata, np.zeros_like(xdata), \"|\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum-likelihood fits \n",
    "\n",
    "Maximum-likelihood fits are the state-of-the-art when it comes to fitting models to data. They can be applied to unbinned and binned data, i.e. histograms:\n",
    "\n",
    "* Unbinned fits are the easiest to use, because they can be apply directly to the raw sample, but they become slow when the sample size is large.\n",
    "\n",
    "* Binned fits require you to appropriately bin the data. The binning has to be fine enough to retain all essential information. This has the advantage, however, that binned fits are much faster when the sample size is large.\n",
    "\n",
    "You will learn in full detail about maximum-likelihood fits in a few weeks in the statistics course, so don't worry if not everything makes sense yet. We will focus here mainly on how to use them and leave the mathematical background for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbinned maximum-likelihood fit\n",
    "\n",
    "Unbinned fits are ideal when the data samples are not too large or very high dimensional. With unbinned fits, there is no need to worry about the appropriate binning of the data. However, these fits are inefficient when the samples are very large and can become numerically unstable, too. In such cases, binned fits are the better choice.\n",
    "\n",
    "The cost function of an unbinned maximum-likelihood fit is quite simple, it is the sum of the logarithm of the pdf, evaluated at each sample point. The entire cost function is then multiplied by -1 to turn maximimization into minimization because these are easier for a computer to deal with. You can easily write this cost function yourself, but a naive implementation will suffer from instabilities when the pdf becomes locally zero. The Minuit implementation mitigates the instabilities to some extend.\n",
    "\n",
    "To perform the unbinned fit you need to provide the pdf of the model, which must be vectorized, i.e. you need to provide a numpy `ufunc`. The pdf must be normalized, which means that the integral over the sample value range must be a constant for any combination of model parameters.\n",
    "\n",
    "In our example case with a gaussian peak over a constant background, the model pdf is the weighted sum of the normal and uniform pdfs. The parameters are $z$ (the weight), $\\mu$ and $\\sigma$ of the normal distribution. The uniform distribution is parameter-free. Remember that the cost function in iminuit automatically detects the parameter names.\n",
    "\n",
    "It is important to put appropriate limits on the parameters, so that the problem does not become mathematically undefined. In this case, appropriate limits are e.g. $0 \\lt z \\lt 1$ , $-1 \\lt \\mu \\lt 1$  , $\\sigma \\gt 0$.\n",
    "\n",
    "Now, we have everything that we need and can try the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pdf(x, z, mu, sigma):\n",
    "    return (z * norm.pdf(x, mu, sigma) +\n",
    "            (1 - z) * uniform.pdf(x, xrange[0], xrange[1] - xrange[0])) #note how useful the ,pdf are\n",
    "\n",
    "c = cost.UnbinnedNLL(xdata, model_pdf)\n",
    "\n",
    "m = Minuit(c, z=0.4, mu=0, sigma=0.2)\n",
    "m.limits[\"z\"] = (0, 1)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.limits[\"sigma\"] = (0, None)\n",
    "\n",
    "m.migrad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "xm = np.linspace(xe[0], xe[-1])\n",
    "plt.plot(xm, model_pdf(xm, *[p.value for p in m.init_params]) * len(xdata) * dx[0],\n",
    "         ls=\":\", label=\"init\")\n",
    "plt.plot(xm, model_pdf(xm, *m.values) * len(xdata) * dx[0], label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended unbinned maximum-likelihood fit\n",
    "\n",
    "An important variant of the unbinned maximum-likelihood fit is described by Roger Barlow, *Nucl.Instrum.Meth.A 297 (1990) 496-506*, the extended unbinned maximum-likelihood fit. The extended unbinned maximum-likelihood fit is used if both the shape and the integral of the density are of interest. This is often true in the world of particle physics where we are trying to estimate physical observables, such as a cross-section or yield.\n",
    "\n",
    "The model in this case has to return the integral of the density and the density itself, which again must be vectorized. The parameters in this case are $n_{sig}$  (integral of the signal density), $n_{bkg}$ (integral of the uniform density), and $\\mu$ and $\\sigma$ of the normal distribution. Again, these parameters need limits so that the problem is mathematically defined:\n",
    "\n",
    "* $n_{sig} \\gt 0$\n",
    "* $n_{bkg} \\gt 0$\n",
    "* $-1 \\lt \\mu \\lt 1$\n",
    "* $\\sigma \\gt 0$\n",
    "\n",
    "Compared to the previous case, we have one more parameter to fit. This is common to extended fits.\n",
    "\n",
    "So, let's try to run the extended fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density(x, nsig, nbkg, mu, sigma):\n",
    "    return nsig + nbkg, (nsig * norm.pdf(x, mu, sigma) +\n",
    "        nbkg * uniform.pdf(x, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedUnbinnedNLL(xdata, model_density)\n",
    "\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted values and the uncertainty estimates for $\\mu$ and $\\sigma$  are identical to the ordinary unbinned maximum-likelihood fit, which tells us that the extended fit found the same minimum as before and \"performs just as well\" for these parameters. But of course, we get the additional integrals.\n",
    "\n",
    "Now, let's plot our result again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "xm = np.linspace(xe[0], xe[-1])\n",
    "plt.plot(xm, model_density(xm, *[p.value for p in m.init_params])[1] * dx[0],\n",
    "         ls=\":\", label=\"init\")\n",
    "plt.plot(xm, model_density(xm, *m.values)[1] * dx[0], label=\"fit\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binned maximum-likelihood fit\n",
    "\n",
    "Binned fits are computationally more efficient and numerically more stable when samples are large. The caveat is that one has to choose an appropriate binning. The binning should be fine enough so that the essential information in the original is retained.\n",
    "\n",
    "In our example, the sample that we want to investigate is large enough and 50 bins are fine enough to retain all information. The maximum-likelihood method applied to binned data gives correct results even if bins have no entries, so choosing a binning that is very fine is not an issue. It just increases the computational cost.\n",
    "\n",
    "The different cost functions for binned fits implemented in Minuit assume that the bin contents are independently Poisson distributed around an unknown expected value per bin. This is correct for ordinary histograms. In more detail, the cost function for a binned maximum-likelihood fit amounts to the sum of the logarithms of Poisson probabilities for the observed counts as a function of the predicted counts in this case. This is then again multiplied by -1 to turn maximization into minimization. However, instead of a pdf, you need to provide a cdf in this case, which must be vectorized as well. Note that you can approximate the cdf as the bin-width times the pdf evaluated at the bin center if it is difficult to calculate. But this is of course only an approxmiation. Using the calculated cdf would be exact.\n",
    "\n",
    "So let's define our cdf and run the binned fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cdf(xe, z, mu, sigma):\n",
    "    return (z * norm.cdf(xe, mu, sigma) +\n",
    "            (1-z) * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.BinnedNLL(n, xe, model_cdf)\n",
    "\n",
    "m = Minuit(c, z=0.4, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"z\"] = (0, 1)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.limits[\"sigma\"] = (0, None)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the fitted values and the uncertainty estimates for $\\mu$ and $\\sigma$ are not identical to the unbinned fit, but very close. For practical purposes, the results are equivalent. This shows that the binning is fine enough to retain the essential information in the original data.\n",
    "\n",
    "Let's plot our binned result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_cdf(xe, *[p.value for p in m.init_params])) * len(xdata), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_cdf(xe, *m.values)) * len(xdata), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended binned maximum-likelihood fit\n",
    "The binned extended maximum-likelihood fit is strictly the binned equivalent of the extended unbinned fit. For the cost function, one sums the logarithm of Poisson probabilities for the observed counts as a function of the predicted counts in this case, again times -1 to turn maximization into minimization.\n",
    "\n",
    "As before, instead of a density, you need to provide a cdf for the binned case, which must also be vectorized. There is no need to separately return the total integral like in the unbinned case. The parameters are the same as in the unbinned extended fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density_cdf(xe, nsig, nbkg, mu, sigma):\n",
    "    return (nsig * norm.cdf(xe, mu, sigma) +\n",
    "            nbkg * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedBinnedNLL(n, xe, model_density_cdf)\n",
    "\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary masking\n",
    "\n",
    "In complicated binned fits with one or multiple peak(s) and background, it is sometimes useful to fit in several stages. One typically starts by masking the signal region, to fit only the background region.\n",
    "\n",
    "This can be used to set up sensible values for a fit or to perform a blind analaysis.\n",
    "\n",
    "The cost functions have a mask attribute for this purpose. We demonstrate the use of the mask with an extended binned fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density_cdf(xe, nsig, nbkg, mu, sigma):\n",
    "    return (nsig * norm.cdf(xe, mu, sigma) +\n",
    "            nbkg * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedBinnedNLL(n, xe, model_density_cdf)\n",
    "\n",
    "# we set the signal amplitude to zero and fix all signal parameters\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.fixed[\"nsig\", \"mu\", \"sigma\"] = True\n",
    "\n",
    "# we temporarily mask out the signal\n",
    "c.mask = (cx < -0.5) | (0.5 < cx)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our background result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fix the background and fit only the signal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mask = None # remove mask\n",
    "m.fixed = False # release all parameters\n",
    "m.fixed[\"nbkg\"] = True # fix background amplitude\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we release all parameters and fit again to get the correct uncertainty estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fixed = None\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now plot the full result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result, of course. Since this was an easy problem, we did not need these extra steps, but doing this is usually helpful to fit lots of histograms without adjusting each fit manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted histograms\n",
    "The cost functions for binned data also support weighted histograms. Just pass an array with the shape `(n, 2)` instead of `(n,)` as the first argument, where the first number of each pair is the sum of weights and the second is the sum of weights squared, i.e. an estimate of the variance of that bin value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares fit\n",
    "\n",
    "In addition, a cost function for a general weighted least-squares fit, aka chi-square fit, is included. In statistics this is also called non-linear regression.\n",
    "\n",
    "In this case you need to provide a model that predicts the $y$-values as a function of the $x$-values and the parameters, and the fit additionally needs estimates of the $y$-errors. If those are wrong, the fit may be biased. If your data has errors on the $x$-values as well, check out the tutorial about [automatic differentiation](https://iminuit.readthedocs.io/en/stable/notebooks/automatic_differentiation.html), which includes an application of that to such fits.\n",
    "\n",
    "So, let's define our model, generate some random data and plot it vs the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a + b * x ** 2\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "yt = model(x, 1, 2)\n",
    "ye = 0.4 * x**5 + 0.1\n",
    "y = rng.normal(yt, ye)\n",
    "\n",
    "plt.plot(x, yt, ls=\"--\", label=\"truth\")\n",
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the least-squares cost function and perform the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.LeastSquares(x, y, ye, model)\n",
    "\n",
    "m = Minuit(c, a=0, b=0)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at our fit result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust least-squares fit\n",
    "\n",
    "The built-in least-squares function also supports robust fitting with alternative loss functions. See the documentation of `iminuit.cost.LeastSquares` for details. There are further built-in loss functions but users can also pass their own. The built-in loss functions are:\n",
    "\n",
    "* `linear` (default): gives ordinary weighted least-squares\n",
    "\n",
    "* `soft_l1`: quadratic ordinary loss for small deviations $(\\ll 1\\sigma)$, linear loss for large deviations $(\\gg 1\\sigma)$, and smooth interpolation in between\n",
    "\n",
    "Let’s create one outlier and see what happens with the ordinary loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a + b * x ** 2\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "yt = model(x, 1, 2)\n",
    "ye = 0.4 * x**5 + 0.1\n",
    "y = rng.normal(yt, ye)\n",
    "y[3] = 3 # generate an outlier\n",
    "\n",
    "c = cost.LeastSquares(x, y, ye, model)\n",
    "\n",
    "m = Minuit(c, a=0, b=0)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is distorted by the outlier. We can repair this with the `soft_l1` loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.loss = \"soft_l1\"\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is now practically identical as in the previous case without an outlier, which demonstrates the power of this method.\n",
    "\n",
    "As you can see, robust fitting is a very powerful if the data are contaminated with small amounts of outliers. It comes with a price, however, because the uncertainties are in general larger. To see this, compare the estimated uncertainty of the parameter `b`, which was 0.15 and is now 0.23.\n",
    "\n",
    "We can actually do better by manually removing the point, through using the mask attribute, and switching back to the ordinary loss. **However, you should always be careful removing any data without a good reason.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mask = c.x != c.x[3]\n",
    "c.loss = \"linear\"\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the uncertainty on $b$ is back to 0.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise ##\n",
    "\n",
    "Let's have a look at a slightly longer exercise that is very close to real data analaysis. The data in `data1` contains a signal on a falling background. Find where this signal is and characterise it, i.e. finds its location, width and how many events it contains. Finally display it clearly - for example you may want to subtract the background and you may want to think about how to best display the uncertainties in your characterisation.\n",
    "    \n",
    "*Note:* These data are in pickle format and so you will have to learn how to use pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "# Advanced Topics\n",
    "    \n",
    "If you want to learn more, the iminuit doumentation also has excellent tutorials on:\n",
    "    \n",
    "- Fits with shared parameters\n",
    "- Fit a PDF with conditional variables\n",
    "\n",
    "We recommend that you do these when you have time because they are very useful for data science purposes. In fact, all of the [tutorials](https://iminuit.readthedocs.io/en/stable/tutorials.html) are excellent and can be quite useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
